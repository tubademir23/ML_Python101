{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import pandas with the alias pd\n",
    "import pandas as pd\n",
    "\n",
    "# Load TSV using the sep keyword argument to set delimiter\n",
    "data = pd.read_csv('vt_tax_data_2016.tsv', sep='\\t')\n",
    "\n",
    "# Plot the total number of tax returns by income group\n",
    "counts = data.groupby(\"agi_stub\").N1.sum()\n",
    "counts.plot.bar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create list of columns to use\n",
    "cols = [\"zipcode\", \"agi_stub\", \"mars1\", \"MARS2\", \"NUMDEP\"]\n",
    "\n",
    "# Create data frame from csv using only selected columns\n",
    "data = pd.read_csv(\"vt_tax_data_2016.csv\", usecols=cols)\n",
    "\n",
    "# View counts of dependents and tax returns by income level\n",
    "print(data.groupby(\"agi_stub\").sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create data frame of next 500 rows with labeled columns\n",
    "vt_data_next500 = pd.read_csv(\"vt_tax_data_2016.csv\", \n",
    "                       \t\t  nrows=500,\n",
    "                       \t\t  skiprows=500,\n",
    "                       \t\t  header=None,\n",
    "                       \t\t  names=vt_data_first500.columns)\n",
    "\n",
    "# View the Vermont data frames to confirm they're different\n",
    "print(vt_data_first500.head())\n",
    "print(vt_data_next500.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dict specifying data types for agi_stub and zipcode\n",
    "data_types = {'agi_stub':\"category\",\n",
    "\t\t\t  'zipcode':\"str\"}\n",
    "\n",
    "# Load csv using dtype to set correct data types\n",
    "data = pd.read_csv(\"vt_tax_data_2016.csv\", dtype=data_types)\n",
    "\n",
    "# Print data types of resulting frame\n",
    "print(data.dtypes.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dict specifying that 0s in zipcode are NA values\n",
    "null_values = {'zipcode':0}\n",
    "\n",
    "# Load csv using na_values keyword argument\n",
    "data = pd.read_csv(\"vt_tax_data_2016.csv\", \n",
    "                na_values=null_values)\n",
    "\n",
    "# View rows with NA ZIP codes\n",
    "print(data[data.zipcode.isna()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "  # Set warn_bad_lines to issue warnings about bad records\n",
    "  data = pd.read_csv(\"vt_tax_data_2016_corrupt.csv\", \n",
    "                     error_bad_lines=False, \n",
    "                     warn_bad_lines=True,)\n",
    "  \n",
    "  # View first 5 records\n",
    "  print(data.head())\n",
    "  \n",
    "except pd.io.common.CParserError:\n",
    "    print(\"Your data contained rows that could not be parsed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pandas as pd\n",
    "import pandas as pd\n",
    "\n",
    "# Read spreadsheet and assign it to survey_responses\n",
    "survey_responses = pd.read_excel('fcc_survey.xlsx')\n",
    "\n",
    "# View the head of the data frame\n",
    "print(survey_responses.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create string of lettered columns to load\n",
    "col_string = \"AD, AW:BA\"\n",
    "\n",
    "# Load data with skiprows and usecols set\n",
    "survey_responses = pd.read_excel(\"fcc_survey_headers.xlsx\", \n",
    "                                 skiprows=2, \n",
    "                                 usecols=col_string)\n",
    "\n",
    "# View the names of the columns selected\n",
    "print(survey_responses.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create df from second worksheet by referencing its position\n",
    "responses_2017 = pd.read_excel(\"fcc_survey.xlsx\",\n",
    "                               sheet_name=1)\n",
    "\n",
    "# Graph where people would like to get a developer job\n",
    "job_prefs = responses_2017.groupby(\"JobPref\").JobPref.count()\n",
    "job_prefs.plot.barh()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create df from second worksheet by referencing its name\n",
    "responses_2017 = pd.read_excel(\"fcc_survey.xlsx\",\n",
    "                               sheet_name='2017')\n",
    "\n",
    "# Graph where people would like to get a developer job\n",
    "job_prefs = responses_2017.groupby(\"JobPref\").JobPref.count()\n",
    "job_prefs.plot.barh()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load all sheets in the Excel file\n",
    "all_survey_data = pd.read_excel(\"fcc_survey.xlsx\",\n",
    "                                sheet_name=[0,'2017'])\n",
    "\n",
    "# View the sheet names in all_survey_data\n",
    "print(all_survey_data.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load all sheets in the Excel file\n",
    "all_survey_data = pd.read_excel(\"fcc_survey.xlsx\", sheet_name=None)\n",
    "\n",
    "# View the sheet names in all_survey_data\n",
    "print(all_survey_data.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an empty data frame\n",
    "all_responses = pd.DataFrame()\n",
    "\n",
    "# Set up for loop to iterate through values in responses\n",
    "for df in responses.values():\n",
    "  # Print the number of rows being added\n",
    "  print(\"Adding {} rows\".format(df.shape[0]))\n",
    "  # Append df to all_responses, assign result\n",
    "  all_responses = all_responses.append(df)\n",
    "\n",
    "# Graph employment statuses in sample\n",
    "counts = all_responses.groupby(\"EmploymentStatus\").EmploymentStatus.count()\n",
    "counts.plot.barh()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set dtype to load appropriate column(s) as Boolean data\n",
    "survey_data = pd.read_excel(\"fcc_survey_subset.xlsx\",\n",
    "                            dtype={'HasDebt':bool})\n",
    "\n",
    "# View financial burdens by Boolean group\n",
    "print(survey_data.groupby('HasDebt').sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load file with Yes as a True value and No as a False value\n",
    "survey_subset = pd.read_excel(\"fcc_survey_yn_data.xlsx\",\n",
    "                              dtype={\"HasDebt\": bool,\n",
    "                              \"AttendedBootCampYesNo\": bool},\n",
    "                              true_values=['Yes'],\n",
    "                              false_values=['No'])\n",
    "\n",
    "# View the data\n",
    "print(survey_subset.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load file, with Part1StartTime parsed as datetime data\n",
    "survey_data = pd.read_excel(\"fcc_survey.xlsx\",\n",
    "                            parse_dates=['Part1StartTime'])\n",
    "\n",
    "# Print first few values of Part1StartTime\n",
    "print(survey_data.Part1StartTime.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dict of columns to combine into new datetime column\n",
    "datetime_cols = {\"Part2Start\": ['Part2StartDate', 'Part2StartTime']}\n",
    "\n",
    "\n",
    "# Load file, supplying the dict to parse_dates\n",
    "survey_data = pd.read_excel(\"fcc_survey_dts.xlsx\",\n",
    "                            parse_dates=datetime_cols)\n",
    "\n",
    "# View summary statistics about Part2Start\n",
    "print(survey_data.Part2Start.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Part2EndTime format =>  %m%d%Y %H:%M:%S"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse datetimes and assign result back to Part2EndTime\n",
    "survey_data[\"Part2EndTime\"] = pd.to_datetime(survey_data[\"Part2EndTime\"],\n",
    "                                   format='%m%d%Y %H:%M:%S')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CHapter 3 Importing Data from Databases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import sqlalchemy's create_engine() function\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "# Create the database engine\n",
    "engine = create_engine('sqlite:///data.db')\n",
    "\n",
    "# View the tables in the database\n",
    "print(engine.table_names())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load libraries\n",
    "import pandas as pd\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "# Create the database engine\n",
    "engine = create_engine('sqlite:///data.db')\n",
    "\n",
    "# Load hpd311calls without any SQL\n",
    "df = pd.read_sql_query(\"SELECT * FROM hpd311calls\", engine)\n",
    "\n",
    "# View the first few rows of data\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the database engine\n",
    "engine = create_engine(\"sqlite:///data.db\")\n",
    "\n",
    "# Create a SQL query to load the entire weather table\n",
    "query = \"\"\"\n",
    "SELECT * \n",
    "  FROM weather;\n",
    "\"\"\"\n",
    "\n",
    "# Load weather with the SQL query\n",
    "weather = pd.read_sql_query(query, engine)\n",
    "\n",
    "# View the first few rows of data\n",
    "print(weather.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create database engine for data.db\n",
    "engine = engine = create_engine(\"sqlite:///data.db\")\n",
    "\n",
    "# Write query to get date, tmax, and tmin from weather\n",
    "query = \"\"\"\n",
    "SELECT date, \n",
    "       tmax, \n",
    "       tmin\n",
    "  FROM weather;\n",
    "\"\"\"\n",
    "\n",
    "# Make a data frame by passing query and engine to read_sql()\n",
    "temperatures = pd.read_sql(query,engine)\n",
    "\n",
    "# View the resulting data frame\n",
    "print(temperatures)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create query to get hpd311calls records about safety\n",
    "query = \"\"\"SELECT *\n",
    "\t\t     FROM hpd311calls\n",
    "            WHERE complaint_type = 'SAFETY';\"\"\"\n",
    "\n",
    "# Query the database and assign result to safety_calls\n",
    "safety_calls = pd.read_sql(query, engine)\n",
    "\n",
    "# Graph the number of safety calls by borough\n",
    "call_counts = safety_calls.groupby('borough').unique_key.count()\n",
    "call_counts.plot.barh()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create query for records with max temps <= 32 or snow >= 1\n",
    "query = \"\"\"\n",
    "SELECT *\n",
    "  FROM weather\n",
    "  where tmax <=32\n",
    "  or snow >=1;\n",
    "\"\"\"\n",
    "\n",
    "# Query database and assign result to wintry_days\n",
    "wintry_days = pd.read_sql(query, engine)\n",
    "\n",
    "# View summary stats about the temperatures\n",
    "print(wintry_days.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create query for unique combinations of borough and complaint_type\n",
    "query = \"\"\"\n",
    "SELECT distinct borough, \n",
    "       complaint_type\n",
    "  from hpd311calls;\n",
    "\"\"\"\n",
    "\n",
    "# Load results of query to a data frame\n",
    "issues_and_boros = pd.read_sql(query, engine)\n",
    "\n",
    "# Check assumption about issues and boroughs\n",
    "print(issues_and_boros)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create query to get call counts by complaint_type\n",
    "query = \"\"\"\n",
    "select complaint_type, \n",
    "     count(*)\n",
    "  FROM hpd311calls\n",
    "  group by complaint_type;\n",
    "\"\"\"\n",
    "\n",
    "# Create data frame of call counts by issue\n",
    "calls_by_issue = pd.read_sql(query, engine)\n",
    "\n",
    "# Graph the number of calls for each housing issue\n",
    "calls_by_issue.plot.barh(x=\"complaint_type\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a query to get month and max tmax by month\n",
    "query = \"\"\"\n",
    "SELECT month, \n",
    "       MAX(tmax)\n",
    "  FROM weather \n",
    " GROUP BY month;\"\"\"\n",
    "\n",
    "# Get data frame of monthly weather stats\n",
    "weather_by_month = pd.read_sql(query, engine)\n",
    "\n",
    "# View weather stats by month\n",
    "print(weather_by_month)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query to join weather to call records by date columns\n",
    "query = \"\"\"\n",
    "SELECT * \n",
    "  FROM hpd311calls\n",
    "  JOIN weather \n",
    "  ON hpd311calls.created_date = weather.date;\n",
    "\"\"\"\n",
    "\n",
    "# Create data frame of joined tables\n",
    "calls_with_weather = pd.read_sql(query,engine)\n",
    "\n",
    "# View the data frame to make sure all columns were joined\n",
    "print(calls_with_weather.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query to get water leak calls and daily precipitation\n",
    "query = \"\"\"\n",
    "SELECT hpd311calls.*, weather.prcp\n",
    "  FROM hpd311calls\n",
    "  JOIN weather\n",
    "    ON hpd311calls.created_date = weather.date\n",
    "  ____ hpd311calls.____ = ____;\"\"\"\n",
    "\n",
    "# Load query results into the leak_calls data frame\n",
    "leak_calls = pd.read_sql(query, engine)\n",
    "\n",
    "# View the data frame\n",
    "print(leak_calls.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query to get water leak calls and daily precipitation\n",
    "query = \"\"\"\n",
    "SELECT hpd311calls.*, weather.prcp\n",
    "  FROM hpd311calls\n",
    "  JOIN weather\n",
    "    ON hpd311calls.created_date = weather.date\n",
    "  where hpd311calls.complaint_type = 'WATER LEAK';\"\"\"\n",
    "\n",
    "# Load query results into the leak_calls data frame\n",
    "leak_calls = pd.read_sql(query, engine)\n",
    "\n",
    "# View the data frame\n",
    "print(leak_calls.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query to get heat/hot water call counts by created_date\n",
    "query = \"\"\"\n",
    "SELECT hpd311calls.created_date, \n",
    "       count(*)\n",
    "  FROM hpd311calls \n",
    "  where hpd311calls.complaint_type = 'HEAT/HOT WATER'\n",
    "  group by hpd311calls.created_date;\n",
    "\"\"\"\n",
    "\n",
    "# Query database and save results as df\n",
    "df = pd.read_sql(query, engine)\n",
    "\n",
    "# View first 5 records\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Chapter 4: Introduction to JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pandas as pd\n",
    "import pandas as pd\n",
    "\n",
    "# Load the daily report to a data frame\n",
    "pop_in_shelters = pd.read_json('dhs_daily_report.json')\n",
    "\n",
    "# View summary stats about pop_in_shelters\n",
    "print(pop_in_shelters.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    # Load the JSON with orient specified\n",
    "    df = pd.read_json(\"dhs_report_reformatted.json\",\n",
    "    orient=\"split\")\n",
    "    \n",
    "    # Plot total population in shelters over time\n",
    "    df[\"date_of_census\"] = pd.to_datetime(df[\"date_of_census\"])\n",
    "    df.plot(x=\"date_of_census\", \n",
    "            y=\"total_individuals_in_shelter\")\n",
    "    plt.show()\n",
    "    \n",
    "except ValueError:\n",
    "    print(\"pandas could not parse the JSON.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Bravo! Check the response format whenever you work with a new API -- chances are the data you need is nested under a dictionary key, like here.\n",
    "One important note: to avoid problems like hitting usage limits or potential connection issues, this course mimics API requests in the background, and, if you set up the code right, returns the corresponding response object. The code you'll write looks exactly like what you would use to interact with the actual API, though. If you're curious about how this works, check out this GitHub repo.\n",
    "https://github.com/adrian-soto/mock-request\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "api_url = \"https://api.yelp.com/v3/businesses/search\"\n",
    "\n",
    "# Get data about NYC cafes from the Yelp API\n",
    "response = requests.get(api_url, \n",
    "                        headers=headers, \n",
    "                        params=params)\n",
    "\n",
    "# Extract JSON data from the response\n",
    "data = response.json()\n",
    "\n",
    "# Load data to a data frame\n",
    "cafes = pd.DataFrame(data[\"businesses\"])\n",
    "\n",
    "# View the data's dtypes\n",
    "print(cafes.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dictionary to query API for cafes in NYC\n",
    "parameters = {'term':'cafe',\n",
    "          \t  'location':'NYC'}\n",
    "\n",
    "headers = {\"Authorization\": \"Bearer {}\".format(api_key)}\n",
    "\n",
    "# Query the Yelp API with headers and params set\n",
    "response = requests.get(api_url,\n",
    "               params=parameters,\n",
    "               headers=headers)\n",
    "\n",
    "# Extract JSON data from response\n",
    "data = response.json()\n",
    "\n",
    "# Load \"businesses\" values to a data frame and print head\n",
    "cafes = pd.DataFrame(data[\"businesses\"])\n",
    "\n",
    "print(cafes.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dictionary that passes Authorization and key string\n",
    "headers = {'Authorization': \"Bearer {}\".format(api_key)}\n",
    "\n",
    "# Query the Yelp API with headers and params set\n",
    "response = requests.get(api_url,\n",
    "headers=headers,\n",
    "params=params)\n",
    "\n",
    "# Extract JSON data from response\n",
    "data = response.json()\n",
    "\n",
    "# Load \"businesses\" values to a data frame and print names\n",
    "cafes =  pd.DataFrame(data['businesses'])\n",
    "print(cafes.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "A feature of JSON data is that it can be nested: an attribute's value can consist of attribute-value pairs. This nested data is more useful unpacked, or flattened, into its own data frame columns. The pandas.io.json submodule has a function, json_normalize(), that does exactly this.\n",
    "\n",
    "The Yelp API response data is nested. Your job is to flatten out the next level of data in the coordinates and location columns.\n",
    "\n",
    "pandas (as pd) and requests have been imported. The results of the API call are stored as response.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load json_normalize()\n",
    "from pandas.io.json import json_normalize\n",
    "\n",
    "# Isolate the JSON data from the API response\n",
    "data = response.json()\n",
    "\n",
    "# Flatten business data into a data frame, replace separator\n",
    "cafes = json_normalize(data[\"businesses\"],\n",
    "                       sep=\"_\")\n",
    "\n",
    "# View data\n",
    "print(cafes.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify record path to get categories data\n",
    "flat_cafes = json_normalize(data[\"businesses\"],\n",
    "                            sep=\"_\",\n",
    "                    \t\trecord_path=\"categories\")\n",
    "\n",
    "# View the data\n",
    "print(flat_cafes.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load other business attributes and set meta prefix\n",
    "flat_cafes = json_normalize(data[\"businesses\"],\n",
    "                            sep=\"_\",\n",
    "                            record_path=\"categories\",\n",
    "                            meta=[\"name\",\n",
    "                                    \"alias\",\n",
    "                                    \"rating\",\n",
    "                                    [\"coordinates\", \"latitude\"],\n",
    "                                    [\"coordinates\", \"longitude\"]],\n",
    "                            meta_prefix=\"biz_\")\n",
    "\n",
    "# View the data\n",
    "print(flat_cafes.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Combining multiple datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add an offset parameter to get cafes 51-100\n",
    "params = {\"term\": \"cafe\", \n",
    "          \"location\": \"NYC\",\n",
    "          \"sort_by\": \"rating\", \n",
    "          \"limit\": 50,\n",
    "          \"offset\": 50}\n",
    "\n",
    "result = requests.get(api_url, headers=headers, params=params)\n",
    "next_50_cafes = json_normalize(result.json()[\"businesses\"])\n",
    "\n",
    "# Append the results, setting ignore_index to renumber rows\n",
    "cafes = top_50_cafes.append(next_50_cafes, ignore_index=True)\n",
    "\n",
    "# Print shape of cafes\n",
    "print(cafes.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add an offset parameter to get cafes 51-100\n",
    "params = {\"term\": \"cafe\", \n",
    "          \"location\": \"NYC\",\n",
    "          \"sort_by\": \"rating\", \n",
    "          \"limit\": 50,\n",
    "          \"offset\": 50}\n",
    "\n",
    "result = requests.get(api_url, headers=headers, params=params)\n",
    "next_50_cafes = json_normalize(result.json()[\"businesses\"])\n",
    "\n",
    "# Append the results, setting ignore_index to renumber rows\n",
    "cafes = top_50_cafes.append(next_50_cafes, ignore_index=True)\n",
    "\n",
    "# Print shape of cafes\n",
    "print(cafes.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge crosswalk into cafes on their zip code fields\n",
    "cafes_with_pumas = cafes.merge(crosswalk, left_on='location_zip_code' ,right_on='zipcode')\n",
    "\n",
    "\n",
    "\n",
    "# Merge pop_data into cafes_with_pumas on puma field\n",
    "cafes_with_pop = cafes_with_pumas.merge(pop_data, left_on='puma', right_on='puma')\n",
    "\n",
    "# View the data\n",
    "print(cafes_with_pop.head())"
   ]
  }
 ]
}